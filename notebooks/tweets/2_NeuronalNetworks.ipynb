{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D1anb1ytuz2k"
      },
      "source": [
        "<a name=\"top\"> <h1>02. Neuronal Networks</h1> <a>\n",
        "\n",
        "<p>Análisis de sentimiento: Tweets<br />\n",
        "<strong>Trabajo de Fin de Master</strong><br />\n",
        "<strong>Master Universitario en Ciencia de Datos</strong></p>\n",
        "\n",
        "<p>&nbsp;</p>\n",
        "\n",
        "<p style=\"text-align:right\">V&iacute;ctor Viloria V&aacute;zquez (<em>victor.viloria@cunef.edu</em>)</p>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MGnWP0EVuz2p"
      },
      "source": [
        "<hr style=\"border:1px solid gray\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V-c-ExD8uz2y"
      },
      "source": [
        "### Estructura"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4y506okXuz22"
      },
      "source": [
        "[1. Librerias utilizadas y funciones](#librerias) \n",
        "\n",
        "[2. Introducción ](#introduccion) \n",
        "\n",
        "   - Objetivo de negocio.\n",
        "\n",
        "[3. Yelp Dataset ](#yelp) \n",
        "\n",
        "   - Información del dataset\n",
        "   - Características del dataset\n",
        "\n",
        "\n",
        "[4. Transformación del formato de ficheros](#transformacion) \n",
        "\n",
        "\n",
        "[5. Transformación de datos](#datos)\n",
        "\n",
        "   - Business\n",
        "       - Carga del fichero\n",
        "       - Transformación de los datos\n",
        "       - Exportación de ficheros procesados"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nhcT10vJuz24"
      },
      "source": [
        "<hr style=\"border:1px solid gray\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_mjKoACXuz25"
      },
      "source": [
        "# <a name=\"librerias\"> 1. Librerias utilizadas y funciones <a>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9JBx12Gsuz26"
      },
      "source": [
        "Importamos las librerias a utilizar para el preprocesamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "a3eiEFg5uz3G"
      },
      "outputs": [],
      "source": [
        "# Import libraries.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "import string\n",
        "import pickle\n",
        "\n",
        "# Import neural network libraries.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Dropout, Dense"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U3CG8-gbuz3I"
      },
      "source": [
        "# <a name=\"lectura\"> 2. Lectura del dataframe <a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PJBg5uE4uz3J",
        "outputId": "bb66f4a0-ec10-4503-d36a-f12328d6a8a3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>SentimentText_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id have responded if i were going</td>\n",
              "      <td>0</td>\n",
              "      <td>id responded going</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sooo sad i will miss you here in san diego</td>\n",
              "      <td>2</td>\n",
              "      <td>sooo sad miss san diego</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>my boss is bullying me</td>\n",
              "      <td>2</td>\n",
              "      <td>boss bullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what interview leave me alone</td>\n",
              "      <td>2</td>\n",
              "      <td>interview leave alone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sons of  why couldnt they put them on the rel...</td>\n",
              "      <td>2</td>\n",
              "      <td>sons couldnt put releases already bought</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>some shameless plugging for the best rangers...</td>\n",
              "      <td>0</td>\n",
              "      <td>shameless plugging best rangers forum earth</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2am feedings for the baby are fun when he is a...</td>\n",
              "      <td>1</td>\n",
              "      <td>2am feedings baby fun smiles coos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>soooo high</td>\n",
              "      <td>0</td>\n",
              "      <td>soooo high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>both of you</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>journey wow u just became cooler  hehe is tha...</td>\n",
              "      <td>1</td>\n",
              "      <td>journey wow u became cooler hehe possible</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment  \\\n",
              "0                  id have responded if i were going          0   \n",
              "1         sooo sad i will miss you here in san diego          2   \n",
              "2                             my boss is bullying me          2   \n",
              "3                      what interview leave me alone          2   \n",
              "4   sons of  why couldnt they put them on the rel...          2   \n",
              "5    some shameless plugging for the best rangers...          0   \n",
              "6  2am feedings for the baby are fun when he is a...          1   \n",
              "7                                         soooo high          0   \n",
              "8                                        both of you          0   \n",
              "9   journey wow u just became cooler  hehe is tha...          1   \n",
              "\n",
              "                           SentimentText_clean  \n",
              "0                           id responded going  \n",
              "1                      sooo sad miss san diego  \n",
              "2                                boss bullying  \n",
              "3                        interview leave alone  \n",
              "4     sons couldnt put releases already bought  \n",
              "5  shameless plugging best rangers forum earth  \n",
              "6            2am feedings baby fun smiles coos  \n",
              "7                                   soooo high  \n",
              "8                                               \n",
              "9    journey wow u became cooler hehe possible  "
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Import parquet file.\n",
        "\n",
        "tweets_df = pd.read_parquet('../../data/processed/tweets.parquet')\n",
        "\n",
        "# Show the head of the dataframe.\n",
        "\n",
        "tweets_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Dw3Rbo2zzuOo"
      },
      "outputs": [],
      "source": [
        "# Save in tweets the column \"SentimentText_clean\" and in sentimientos the column \"sentiment\".   \n",
        "\n",
        "tweets = tweets_df[\"SentimentText_clean\"]\n",
        "\n",
        "sentimientos = tweets_df[\"sentiment\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "687/687 [==============================] - 77s 105ms/step - loss: 0.8052 - accuracy: 0.6380 - val_loss: 0.6909 - val_accuracy: 0.7088\n",
            "Epoch 2/10\n",
            "687/687 [==============================] - 72s 105ms/step - loss: 0.5135 - accuracy: 0.7959 - val_loss: 0.7345 - val_accuracy: 0.6980\n",
            "Epoch 3/10\n",
            "687/687 [==============================] - 73s 106ms/step - loss: 0.3260 - accuracy: 0.8790 - val_loss: 0.8553 - val_accuracy: 0.6880\n",
            "Epoch 4/10\n",
            "687/687 [==============================] - 75s 109ms/step - loss: 0.2148 - accuracy: 0.9236 - val_loss: 1.0571 - val_accuracy: 0.6798\n",
            "Epoch 5/10\n",
            "687/687 [==============================] - 74s 107ms/step - loss: 0.1432 - accuracy: 0.9499 - val_loss: 1.2096 - val_accuracy: 0.6604\n",
            "Epoch 6/10\n",
            "687/687 [==============================] - 73s 107ms/step - loss: 0.1090 - accuracy: 0.9627 - val_loss: 1.4571 - val_accuracy: 0.6764\n",
            "Epoch 7/10\n",
            "687/687 [==============================] - 74s 108ms/step - loss: 0.0878 - accuracy: 0.9687 - val_loss: 1.6090 - val_accuracy: 0.6651\n",
            "Epoch 8/10\n",
            "687/687 [==============================] - 74s 107ms/step - loss: 0.0669 - accuracy: 0.9770 - val_loss: 1.7779 - val_accuracy: 0.6620\n",
            "Epoch 9/10\n",
            "687/687 [==============================] - 76s 110ms/step - loss: 0.0596 - accuracy: 0.9786 - val_loss: 1.7913 - val_accuracy: 0.6642\n",
            "Epoch 10/10\n",
            "687/687 [==============================] - 74s 108ms/step - loss: 0.0493 - accuracy: 0.9822 - val_loss: 2.0801 - val_accuracy: 0.6633\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x281d4c698b0>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# Dividir los datos en características y etiquetas\n",
        "X = tweets\n",
        "y = tweets_df['sentiment'].values\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# Crear el Tokenizer y ajustarlo solo a las palabras del conjunto de entrenamiento\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Convertir las secuencias de palabras en secuencias numéricas\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Aplicar padding para que todas las secuencias tengan la misma longitud\n",
        "max_sequence_length = max([len(sequence) for sequence in X_train_sequences])\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Convertir las etiquetas a one-hot encoding\n",
        "num_classes = 3\n",
        "y_train_encoded = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test_encoded = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Crear el modelo de redes neuronales\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_sequence_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train_padded, y_train_encoded, validation_data=(X_test_padded, y_test_encoded), epochs=10, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(\"../../models/nn_tweets.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_sequence_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_text = pd.Series(request.form['text'])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict the sentiment of a tweet.\n",
        "\n",
        "def predict_sentiment(text):\n",
        "\n",
        "    # Convert the text to sequence of words\n",
        "    X_test_sequences = tokenizer.texts_to_sequences([text])\n",
        "\n",
        "    # Pad the sequence\n",
        "    X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(X_test_padded)\n",
        "\n",
        "    # Get the sentiment with the highest probability\n",
        "    sentiment = np.argmax(predictions)\n",
        "\n",
        "    # Put name to the sentiment\n",
        "\n",
        "    if sentiment == 0:\n",
        "        sentiment = 'Neutral',\n",
        "    elif sentiment == 1:\n",
        "        sentiment = 'Positive'\n",
        "    else:\n",
        "        sentiment = 'Negative'\n",
        "\n",
        "\n",
        "    return sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 46ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Negative'"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_sentiment(\"I hate my girlfriend\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "5ruaSfnJz2pO"
      },
      "outputs": [],
      "source": [
        "# Save the tokenizer in models.\n",
        "\n",
        "with open('../../models/tweets_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "cT_bwUmXz94j"
      },
      "outputs": [],
      "source": [
        "max_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "6CX11VGjz_m_"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=len(word_index) + 1, output_dim=64, input_length=max_length),\n",
        "    tf.keras.layers.Conv1D(128, 5, activation=\"relu\"),\n",
        "    tf.keras.layers.GlobalMaxPooling1D(),\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(3, activation=\"softmax\")\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "65TMNdfP0HFG"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIKApT5a0K23",
        "outputId": "a6e9d3c5-2206-4beb-8b62-a4a923d02523"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "687/687 [==============================] - 43s 61ms/step - loss: 0.7896 - accuracy: 0.6477 - val_loss: 0.6829 - val_accuracy: 0.7229\n",
            "Epoch 2/10\n",
            "687/687 [==============================] - 43s 62ms/step - loss: 0.5522 - accuracy: 0.7805 - val_loss: 0.7106 - val_accuracy: 0.7020\n",
            "Epoch 3/10\n",
            "687/687 [==============================] - 42s 61ms/step - loss: 0.3523 - accuracy: 0.8730 - val_loss: 0.8570 - val_accuracy: 0.6846\n",
            "Epoch 4/10\n",
            "687/687 [==============================] - 43s 63ms/step - loss: 0.1924 - accuracy: 0.9351 - val_loss: 1.0696 - val_accuracy: 0.6605\n",
            "Epoch 5/10\n",
            "687/687 [==============================] - 42s 62ms/step - loss: 0.1054 - accuracy: 0.9662 - val_loss: 1.2942 - val_accuracy: 0.6520\n",
            "Epoch 6/10\n",
            "687/687 [==============================] - 43s 63ms/step - loss: 0.0640 - accuracy: 0.9804 - val_loss: 1.6097 - val_accuracy: 0.6498\n",
            "Epoch 7/10\n",
            "687/687 [==============================] - 44s 64ms/step - loss: 0.0450 - accuracy: 0.9870 - val_loss: 1.8108 - val_accuracy: 0.6516\n",
            "Epoch 8/10\n",
            "687/687 [==============================] - 44s 64ms/step - loss: 0.0376 - accuracy: 0.9888 - val_loss: 1.8970 - val_accuracy: 0.6458\n",
            "Epoch 9/10\n",
            "687/687 [==============================] - 47s 68ms/step - loss: 0.0340 - accuracy: 0.9896 - val_loss: 2.0669 - val_accuracy: 0.6449\n",
            "Epoch 10/10\n",
            "687/687 [==============================] - 46s 67ms/step - loss: 0.0356 - accuracy: 0.9879 - val_loss: 2.2525 - val_accuracy: 0.6465\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x281d1ea1160>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Entrenar el modelo\n",
        "model.fit(padded_sequences, sentimientos, epochs=10, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uil2PX2f0xRe",
        "outputId": "2b72caeb-a0b2-449e-ac8d-81527f14242f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "687/687 [==============================] - 22s 29ms/step - loss: 1.0151 - accuracy: 0.4689 - val_loss: 0.7697 - val_accuracy: 0.6631\n",
            "Epoch 2/20\n",
            "687/687 [==============================] - 5s 7ms/step - loss: 0.7289 - accuracy: 0.6988 - val_loss: 0.6758 - val_accuracy: 0.7178\n",
            "Epoch 3/20\n",
            "687/687 [==============================] - 4s 6ms/step - loss: 0.5802 - accuracy: 0.7719 - val_loss: 0.7030 - val_accuracy: 0.7142\n",
            "Epoch 4/20\n",
            "687/687 [==============================] - 4s 7ms/step - loss: 0.4708 - accuracy: 0.8212 - val_loss: 0.7474 - val_accuracy: 0.7033\n",
            "Epoch 5/20\n",
            "687/687 [==============================] - 4s 6ms/step - loss: 0.3847 - accuracy: 0.8555 - val_loss: 0.8447 - val_accuracy: 0.7022\n",
            "Epoch 6/20\n",
            "687/687 [==============================] - 3s 5ms/step - loss: 0.3166 - accuracy: 0.8860 - val_loss: 0.9472 - val_accuracy: 0.6875\n",
            "Epoch 7/20\n",
            "687/687 [==============================] - 5s 7ms/step - loss: 0.2697 - accuracy: 0.9041 - val_loss: 1.0619 - val_accuracy: 0.6789\n",
            "Epoch 8/20\n",
            "687/687 [==============================] - 4s 5ms/step - loss: 0.2288 - accuracy: 0.9206 - val_loss: 1.1757 - val_accuracy: 0.6786\n",
            "Epoch 9/20\n",
            "687/687 [==============================] - 4s 6ms/step - loss: 0.2033 - accuracy: 0.9307 - val_loss: 1.4384 - val_accuracy: 0.6796\n",
            "Epoch 10/20\n",
            "687/687 [==============================] - 4s 6ms/step - loss: 0.1814 - accuracy: 0.9376 - val_loss: 1.4390 - val_accuracy: 0.6776\n",
            "Epoch 11/20\n",
            "687/687 [==============================] - 4s 6ms/step - loss: 0.1624 - accuracy: 0.9444 - val_loss: 1.5350 - val_accuracy: 0.6689\n",
            "Epoch 12/20\n",
            "687/687 [==============================] - 3s 5ms/step - loss: 0.1510 - accuracy: 0.9516 - val_loss: 1.5593 - val_accuracy: 0.6756\n",
            "Epoch 13/20\n",
            "687/687 [==============================] - 4s 6ms/step - loss: 0.1418 - accuracy: 0.9540 - val_loss: 1.6352 - val_accuracy: 0.6716\n",
            "Epoch 14/20\n",
            "687/687 [==============================] - 3s 5ms/step - loss: 0.1309 - accuracy: 0.9577 - val_loss: 1.6850 - val_accuracy: 0.6633\n",
            "Epoch 15/20\n",
            "687/687 [==============================] - 4s 5ms/step - loss: 0.1218 - accuracy: 0.9600 - val_loss: 1.8680 - val_accuracy: 0.6740\n",
            "Epoch 16/20\n",
            "687/687 [==============================] - 4s 6ms/step - loss: 0.1130 - accuracy: 0.9624 - val_loss: 1.9218 - val_accuracy: 0.6684\n",
            "Epoch 17/20\n",
            "687/687 [==============================] - 5s 7ms/step - loss: 0.1064 - accuracy: 0.9665 - val_loss: 2.0797 - val_accuracy: 0.6635\n",
            "Epoch 18/20\n",
            "687/687 [==============================] - 4s 5ms/step - loss: 0.1012 - accuracy: 0.9670 - val_loss: 2.0242 - val_accuracy: 0.6658\n",
            "Epoch 19/20\n",
            "687/687 [==============================] - 4s 5ms/step - loss: 0.0924 - accuracy: 0.9698 - val_loss: 2.2860 - val_accuracy: 0.6640\n",
            "Epoch 20/20\n",
            "687/687 [==============================] - 4s 6ms/step - loss: 0.0915 - accuracy: 0.9703 - val_loss: 2.3402 - val_accuracy: 0.6576\n"
          ]
        }
      ],
      "source": [
        "# Crear el modelo de la red neuronal\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=len(word_index) + 1, output_dim=64, input_length=max_length),\n",
        "    tf.keras.layers.Conv1D(128, 5, activation=\"relu\"),\n",
        "    tf.keras.layers.MaxPooling1D(5),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Conv1D(64, 5, activation=\"relu\"),\n",
        "    tf.keras.layers.GlobalMaxPooling1D(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(3, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Entrenar el modelo con validación cruzada\n",
        "history = model.fit(padded_sequences, sentimientos, epochs=20, validation_split=0.2, batch_size=32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "687/687 [==============================] - 8s 12ms/step - loss: 1.0202 - accuracy: 0.4670 - val_loss: 0.8594 - val_accuracy: 0.5701\n",
            "Epoch 2/20\n",
            "687/687 [==============================] - 8s 12ms/step - loss: 0.8210 - accuracy: 0.6325 - val_loss: 0.7230 - val_accuracy: 0.7004\n",
            "Epoch 3/20\n",
            "687/687 [==============================] - 8s 12ms/step - loss: 0.6926 - accuracy: 0.7123 - val_loss: 0.6940 - val_accuracy: 0.7097\n",
            "Epoch 4/20\n",
            "687/687 [==============================] - 8s 11ms/step - loss: 0.6116 - accuracy: 0.7555 - val_loss: 0.7083 - val_accuracy: 0.6911\n",
            "Epoch 5/20\n",
            "687/687 [==============================] - 8s 12ms/step - loss: 0.5469 - accuracy: 0.7890 - val_loss: 0.7112 - val_accuracy: 0.7097\n",
            "Epoch 6/20\n",
            "687/687 [==============================] - 8s 12ms/step - loss: 0.5012 - accuracy: 0.8103 - val_loss: 0.7398 - val_accuracy: 0.7011\n",
            "Epoch 7/20\n",
            "687/687 [==============================] - 8s 12ms/step - loss: 0.4609 - accuracy: 0.8271 - val_loss: 0.7676 - val_accuracy: 0.6957\n",
            "Epoch 8/20\n",
            "687/687 [==============================] - 8s 12ms/step - loss: 0.4194 - accuracy: 0.8443 - val_loss: 0.7963 - val_accuracy: 0.6816\n",
            "Epoch 9/20\n",
            "687/687 [==============================] - 8s 12ms/step - loss: 0.3914 - accuracy: 0.8554 - val_loss: 0.8238 - val_accuracy: 0.6873\n",
            "Epoch 10/20\n",
            "687/687 [==============================] - 8s 11ms/step - loss: 0.3658 - accuracy: 0.8678 - val_loss: 0.8621 - val_accuracy: 0.6864\n",
            "Epoch 11/20\n",
            "687/687 [==============================] - 8s 12ms/step - loss: 0.3493 - accuracy: 0.8727 - val_loss: 0.8787 - val_accuracy: 0.6816\n",
            "Epoch 12/20\n",
            "687/687 [==============================] - 8s 12ms/step - loss: 0.3304 - accuracy: 0.8797 - val_loss: 0.9105 - val_accuracy: 0.6787\n",
            "Epoch 13/20\n",
            "687/687 [==============================] - 8s 12ms/step - loss: 0.3114 - accuracy: 0.8872 - val_loss: 0.9467 - val_accuracy: 0.6744\n",
            "Epoch 14/20\n",
            "687/687 [==============================] - 8s 12ms/step - loss: 0.2965 - accuracy: 0.8950 - val_loss: 1.0017 - val_accuracy: 0.6725\n",
            "Epoch 15/20\n",
            "687/687 [==============================] - 8s 12ms/step - loss: 0.2823 - accuracy: 0.8996 - val_loss: 1.0567 - val_accuracy: 0.6653\n",
            "Epoch 16/20\n",
            "687/687 [==============================] - 8s 11ms/step - loss: 0.2703 - accuracy: 0.9028 - val_loss: 1.0994 - val_accuracy: 0.6664\n",
            "Epoch 17/20\n",
            "687/687 [==============================] - 8s 12ms/step - loss: 0.2639 - accuracy: 0.9049 - val_loss: 1.1198 - val_accuracy: 0.6680\n",
            "Epoch 18/20\n",
            "687/687 [==============================] - 8s 12ms/step - loss: 0.2463 - accuracy: 0.9128 - val_loss: 1.1710 - val_accuracy: 0.6664\n",
            "Epoch 19/20\n",
            "687/687 [==============================] - 8s 11ms/step - loss: 0.2376 - accuracy: 0.9167 - val_loss: 1.1931 - val_accuracy: 0.6636\n",
            "Epoch 20/20\n",
            "687/687 [==============================] - 8s 11ms/step - loss: 0.2335 - accuracy: 0.9172 - val_loss: 1.2086 - val_accuracy: 0.6642\n"
          ]
        }
      ],
      "source": [
        "# Crear el modelo de la red neuronal\n",
        "model = tf.keras.Sequential([\n",
        "    Embedding(input_dim=len(word_index) + 1, output_dim=64, input_length=max_length),\n",
        "    MaxPooling1D(5),\n",
        "    Dropout(0.5),\n",
        "    Conv1D(64, 5, activation=\"relu\"),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Entrenar el modelo con validación cruzada\n",
        "history = model.fit(padded_sequences, sentimientos, epochs=20, validation_split=0.2, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "IVbumLn21Q8A"
      },
      "outputs": [],
      "source": [
        "model.save(\"../../models/nn_tweets2.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "[[9.80702698e-01 6.14428660e-03 1.31530846e-02]\n",
            " [4.15356159e-01 3.13416566e-03 5.81509709e-01]\n",
            " [9.80702698e-01 6.14428660e-03 1.31530846e-02]\n",
            " [9.87786174e-01 5.32803312e-03 6.88591553e-03]\n",
            " [9.80702698e-01 6.14428660e-03 1.31530846e-02]\n",
            " [9.80702698e-01 6.14428660e-03 1.31530846e-02]\n",
            " [9.80702698e-01 6.14428660e-03 1.31530846e-02]\n",
            " [9.80702698e-01 6.14428660e-03 1.31530846e-02]\n",
            " [9.80702698e-01 6.14428660e-03 1.31530846e-02]\n",
            " [9.95299578e-01 8.20325731e-05 4.61843098e-03]\n",
            " [9.80702698e-01 6.14428660e-03 1.31530846e-02]\n",
            " [9.80702698e-01 6.14428660e-03 1.31530846e-02]\n",
            " [9.99979019e-01 1.52212615e-05 5.67355437e-06]\n",
            " [9.97592211e-01 1.61777937e-03 7.90042512e-04]\n",
            " [9.80702698e-01 6.14428660e-03 1.31530846e-02]\n",
            " [9.80702698e-01 6.14428660e-03 1.31530846e-02]\n",
            " [9.80702698e-01 6.14428660e-03 1.31530836e-02]]\n"
          ]
        }
      ],
      "source": [
        "# Load the model of the neural network.\n",
        "\n",
        "model = tf.keras.models.load_model('../../models/nn_tweets2.h5')\n",
        "\n",
        "# Load the tokenizer.\n",
        "\n",
        "# Input text.\n",
        "\n",
        "text = input(\"Ingrese el texto: \")\n",
        "\n",
        "with open('../../models/tweets_tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "# Convertir las secuencias de palabras en secuencias numéricas\n",
        "text = tokenizer.texts_to_sequences(text)\n",
        "\n",
        "# Aplicar padding para que todas las secuencias tengan la misma longitud\n",
        "max_sequence_length = len(text)\n",
        "text = pad_sequences(text, maxlen=25, padding='post')\n",
        "\n",
        "prediction = model.predict(text)\n",
        "\n",
        "print(prediction)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "TFM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
